import argparse
import logging
from typing import List, Set
from lhotse import Cut, CutSet, load_manifest_lazy
from lhotse.serialization import SequentialJsonlWriter
from textsearch import (
    TextSource,
    SourcedText,
    append_texts,
    create_suffix_array_from_sourced_text,
    filter_texts,
    find_candidate_matches,
    find_close_matches,
    levenshtein_distance,
    texts_to_sourced_texts,
)


def get_args():
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--manifest-in",
        type=str,
        help="""The manifest generated by transcript stage containing book path,
        recordings path and recognition results.
        """,
    )
    parser.add_argument(
        "--manifest-out",
        type=str,
        help="""The file name of the new manifests to write to. 
        """,
    )
    parser.add_argument(
        "--batch-size",
        type=int,
        default=10,
        help="""The number of cuts in a batch.
        """,
    )

    return parser.parse_args()


def _align_worker(
    transcript: Transcript,
    indexes: Tuple[int, int],
    candidate_matches: List[Tuple[int, int]],
) -> Tuple[Tuple[int, int], Tuple[int, int, str, str]]:
    pass


def process_one_batch(batch_cuts: List[Cut], cuts_writer: SequentialJsonlWriter):
    transcripts: List[Transcript] = []
    # contains cut index and supervision index
    transcripts_cut_index: List[Tuple[int, int]] = []
    books: List[TextSource] = []
    book_paths: Set[str] = set()
    query_len = 0

    # transcripts
    for i, cut in enumerate(batch_cuts):
        for j, sup in enumerate(cut.supervisions):
            aligns = {"text": [], "begin_times": []}
            for ali in sup["alignment"]["symbol"]:
                aligns["text"].append(ali[0])
                aligns["begin_times"].append(ali[1])
            transcript = Transcript.from_dict(
                name=sup["id"], d=aligns, use_utf8=False, is_bpe=True
            )
            query_len += transcript.binary_text.size
            transcripts.append(transcript)
            transcripts_cut_index.append((i, j))
        book_paths.add(cut["text_path"])

    # references
    for i, book_path in enumerate(book_paths):
        book_text = open(book_path, "r").read()
        book = TextSource.from_str(
            name=book_path, s=book_text, use_utf8=False, uppercase=True
        )
        books.append(book)

    sourced_transcript_lists = texts_to_sourced_texts(transcripts)
    sourced_transcripts = append_texts(sourced_transcript_lists)

    sourced_book_list = texts_to_sourced_texts(books)
    sourced_books = append_texts(sourced_book_list)
    sourced_books = filter_texts(sourced_books, fn=is_not_punc)

    sourced_text = append_texts([sourced_transcripts, sourced_books])

    suffix_array = create_suffix_array_from_sourced_text(sourced_text)
    close_matches = find_close_matches(suffix_array, query_len, num_close_matches=1)
    candidate_matches = find_candidate_matches(
        close_matches=close_matches,
        text=sourced_text,
        num_candidates=1,
        length_ratio=1.0,
    )

    assert len(transcripts) == len(candidate_matches), (
        len(transcripts),
        len(candidate_matches),
    )

    for i, transcript in enumerate(transcripts):
        query = transcript.binary_text
        query_length = transcript.binary_text.size
        min_cost: float = query_length
        best_alignment: Tuple[float, List[int, int, str]] = None
        best_match: Tuple[int, int] = None
        for j, match in enumerate(candidate_matches[i]):
            assert sourced_text.doc[match[0]] == sourced_text.doc[match[1]], (
                sourced_text.doc[match[0]],
                sourced_text.doc[match[1]],
            )

            start = sourced_text.doc_splits[sourced_text.doc[match[0]]]
            end = sourced_text.doc_splits[sourced_text.doc[match[0]] + 1]
            start = max(start, match[0] - query_length // 4)
            end = min(end, match[1] + query_length // 4)
            target = sourced_text.binary_text[start:end]

            alignment = levenshtein_distance(query=query, target=target)
            if alignment[0] < min_cost:
                min_cost = alignment[0]
                best_alignment = alignment
                best_match = (start, end)

        start = best_match[0] + best_alignment[1][0]
        end = best_match[0] + best_alignment[1][1]

        ref = "".join([chr(i) for i in self.binary_text[start : end + 1]])

        base = sourced_text.doc_splits[sourced_text.doc[start]]
        start_pos = sourced_text.pos[start] - base
        end_pos = sourced_text.pos[end] - base

        batch_cuts[transcripts_cut_index[i][0]]["supervisions"][
            transcripts_cut_index[i][1]
        ]["text"] = ref
        batch_cuts[transcripts_cut_index[i][0]]["supervisions"][
            transcripts_cut_index[i][1]
        ]["align"] = best_alignment[1][2]
        batch_cuts[transcripts_cut_index[i][0]]["supervisions"][
            transcripts_cut_index[i][1]
        ]["begin"] = start_pos
        batch_cuts[transcripts_cut_index[i][0]]["supervisions"][
            transcripts_cut_index[i][1]
        ]["end"] = end_pos

    # cuts_writer.write(new_cut, flush=True)


def main():
    args = get_args()
    raw_cuts = load_manifest_lazy(args.manifest_in)
    cuts_writer = CutSet.open_writer(args.manifest_out, overwrite=False)
    batch_cuts = []
    for i, cut in enumerate(raw_cuts):
        if len(batch_cuts) < args.batch_size:
            batch_cuts.append(cut)
        else:
            process_one_batch(batch_cuts, cuts_writer)
            batch_cuts = []
            logging.info(f"Number of cuts have been processed is {i}")
    if len(batch_cuts):
        process_one_batch(batch_cuts, cuts_writer)
    cuts_writer.close()


if __name__ == "__main__":
    formatter = "%(asctime)s %(levelname)s [%(filename)s:%(lineno)d] %(message)s"
    logging.basicConfig(
        level=logging.INFO,
        format=formatter,
        handlers=[logging.FileHandler("matching.log"), logging.StreamHandler()],
    )

    main()
