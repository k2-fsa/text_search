import argparse
import logging
import numpy as np
from heapq import heappush, heappop
from multiprocessing.pool import Pool
from multiprocessing.pool import ThreadPool
from pathlib import Path
from queue import Queue
from threading import Thread
from typing import Dict, List, Set, Tuple, Union

from lhotse import CutSet, MonoCut, SupervisionSegment, load_manifest_lazy
from lhotse.serialization import SequentialJsonlWriter
from textsearch import (
    AttributeDict,
    TextSource,
    Transcript,
    SourcedText,
    append_texts,
    create_suffix_array,
    filter_texts,
    find_close_matches,
    get_alignments,
    is_punctuation,
    texts_to_sourced_texts,
    split_into_segments,
)


def get_args():
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--manifest-in",
        type=str,
        help="""The manifest generated by transcript stage containing book path,
        recordings path and recognition results.
        """,
    )
    parser.add_argument(
        "--manifest-out",
        type=str,
        help="""The file name of the new manifests to write to. 
        """,
    )
    parser.add_argument(
        "--book-dir",
        type=str,
        help="""The directory of books.
        """,
    )
    parser.add_argument(
        "--batch-size",
        type=int,
        default=10,
        help="""The number of cuts in a batch.
        """,
    )

    return parser.parse_args()


def get_params() -> AttributeDict:
    """Return a dict containing matching parameters.

    All related parameters that are not passed from the commandline
    are saved in the variable `params`.

    Commandline options are merged into `params` after they are parsed, so
    you can also access them via `params`.

    """
    params = AttributeDict(
        {
            "num_workers": 2,
            "num_close_matches": 2,
            "reference_length_difference": 0.1,
            "segment_length": 5000,
            "use_utf8": False,
            "is_bpe": True,
            "use_uppercase": True,
        }
    )

    return params


def dataloader(params: AttributeDict, cuts_queue: Queue, data_queue: Queue):
    while True:
        batch_cuts = cuts_queue.get()
        if batch_cuts is None:
            cuts_queue.put(None)
            break

        # List of transcripts (total number of valid supervisions in the cuts).
        transcripts: List[Transcript] = []

        # Contains cut index and local supervision index
        transcripts_cut_index: List[Tuple[int, int]] = []

        # Constructed from the valid books in the cuts
        books: List[TextSource] = []

        # unique books exist in this batch of cuts
        book_paths: Set[str] = set()
        query_len = 0

        # Construct transcripts
        for i, cut in enumerate(batch_cuts):
            # No text book available, skip this cut.
            if cut.text_path == "":
                logging.warning(f"Skipping {cut.id} due to missing of reference book")
                continue
            for j, sup in enumerate(cut.supervisions):
                # Transcript requires the input to be the dict like this.
                text_list = []
                begin_times_list = []
                for ali in sup.alignment["symbol"]:
                    text_list.append(ali.symbol)
                    begin_times_list.append(ali.start)
                aligns = {"text": text_list, "begin_times": begin_times_list}
                # alignments in a supervision might be empty
                if aligns["text"]:
                    transcript = Transcript.from_dict(
                        name=sup.id,
                        d=aligns,
                        use_utf8=params.use_utf8,
                        is_bpe=params.is_bpe,
                    )
                    query_len += transcript.binary_text.size
                    transcripts.append(transcript)
                    transcripts_cut_index.append((i, j))
            book_paths.add(cut.text_path)

        # Construct references (the books)
        for i, book_path in enumerate(book_paths):
            with open(params.book_dir / book_path, "r") as f:
                book_text = f.read()
                book = TextSource.from_str(
                    name=book_path,
                    s=book_text,
                    use_utf8=params.use_utf8,
                )
                books.append(book)

        if not transcripts:
            continue

        sourced_transcript_lists = texts_to_sourced_texts(
            transcripts, uppercase=params.use_uppercase
        )
        sourced_transcripts = append_texts(sourced_transcript_lists)

        sourced_book_list = texts_to_sourced_texts(
            books, uppercase=params.use_uppercase
        )
        sourced_books = append_texts(sourced_book_list)

        def _is_not_punc(c: np.int32) -> bool:
            return not is_punctuation(chr(int(c)))

        # Removing the punctuation
        sourced_books = filter_texts(sourced_books, fn=_is_not_punc)

        sourced_text = append_texts([sourced_transcripts, sourced_books])

        assert query_len == sourced_text.doc_splits[len(transcripts)], (
            query_len,
            sourced_text.doc_splits[len(transcripts)],
        )
        data_queue.put(
            {
                "query_len": query_len,
                "cuts": batch_cuts,
                "cut_indexes": transcripts_cut_index,
                "sourced_text": sourced_text,
            }
        )
    data_queue.put(None)
    logging.info("dataloader done.")


def aligner(
    params: AttributeDict,
    data_queue: Queue,
    align_queue: Queue,
    thread_pool: ThreadPool,
):
    while True:
        item = data_queue.get()
        if item is None:
            data_queue.put(None)
            break
        sourced_text = item["sourced_text"]
        query_len = item["query_len"]

        logging.info(f"Creating suffix array.")
        suffix_array = create_suffix_array(sourced_text.binary_text)

        logging.info(f"Finding close matches.")
        close_matches = find_close_matches(
            suffix_array, query_len, num_close_matches=params.num_close_matches
        )

        logging.info(f"Getting alignments.")
        alignments = get_alignments(
            sourced_text,
            close_matches,
            segment_length=params.segment_length,
            reference_length_difference=params.reference_length_difference,
            thread_pool=thread_pool,
        )

        assert sourced_text.doc[query_len] == len(alignments), (
            sourced_text.doc[query_len],
            len(alignments),
        )
        assert len(item["cut_indexes"]) == len(alignments), (
            len(item["cut_indexes"]),
            len(alignments),
        )
        align_queue.put(
            {
                "cuts": item["cuts"],
                "cut_indexes": item["cut_indexes"],
                "alignments": alignments,
                "sourced_text": sourced_text,
            }
        )
    align_queue.put(None)
    logging.info(f"aligner done.")


def split_helper(sourced_text, transcripts_cut_index, alignment):
    segments = split_into_segments(sourced_text, alignment)
    return transcripts_cut_index, segments


def splitter(
    params: AttributeDict, align_queue: Queue, write_queue: Queue, process_pool: Pool
):
    while True:
        item = align_queue.get()
        if item is None:
            align_queue.put(None)
            break

        alignments = item["alignments"]
        sourced_text = item["sourced_text"]
        cut_indexes = item["cut_indexes"]

        arguments = []
        for i in range(len(alignments)):
            if alignments[i] is not None:
                arguments.append((sourced_text, cut_indexes[i], alignments[i]))

        logging.info("Splitting into segments.")
        async_results = process_pool.starmap_async(split_helper, arguments)
        results = async_results.get()
        logging.info("Splitting into segments done.")

        write_queue.put({"cuts": item["cuts"], "segments": results})
    write_queue.put(None)
    logging.info(f"splitter done.")


def writer(
    params: AttributeDict, write_queue: Queue, cuts_writer: SequentialJsonlWriter
):
    while True:
        item = write_queue.get()
        if item is None:
            break

        results = item["segments"]
        batch_cuts = item["cuts"]
        cut_segment_index: Dict[str, int] = {}
        cut_list = []
        for item in results:
            cut_indexes = item[0]
            segments = item[1]

            current_cut = batch_cuts[cut_indexes[0]]
            if current_cut.id not in cut_segment_index:
                cut_segment_index[current_cut.id] = 0

            for seg in segments:
                id = f"{current_cut.id}_{cut_segment_index[current_cut.id]}"
                cut_segment_index[current_cut.id] += 1
                supervision = SupervisionSegment(
                    id=id,
                    channel=current_cut.supervisions[cut_indexes[1]].channel,
                    language=current_cut.supervisions[cut_indexes[1]].language,
                    speaker=current_cut.supervisions[cut_indexes[1]].speaker,
                    recording_id=current_cut.recording.id,
                    start=seg["start_time"],
                    duration=seg["duration"],
                    custom={
                        "texts": [seg["ref"], seg["hyp"]],
                        "pre_texts": [seg["pre_ref"], seg["pre_hyp"]],
                        "begin_byte": seg["begin_byte"],
                        "end_byte": seg["end_byte"],
                    },
                )
                cut = MonoCut(
                    id,
                    start=seg["start_time"],
                    duration=seg["duration"],
                    channel=current_cut.channel,
                    supervisions=[supervision],
                    recording=current_cut.recording,
                    custom={"text_path": str(params.book_dir / current_cut.text_path)},
                )
                cut_list.append(cut)

        logging.info(f"Writing results.")
        for cut in cut_list:
            cuts_writer.write(cut, flush=True)
        logging.info(f"Write results done.")
    logging.info(f"writer done.")


def main():
    args = get_args()
    args.book_dir = Path(args.book_dir)
    params = get_params()
    params.update(vars(args))

    raw_cuts = load_manifest_lazy(params.manifest_in)
    cuts_writer = CutSet.open_writer(params.manifest_out, overwrite=True)

    thread_pool = ThreadPool()
    process_pool = Pool()

    cuts_queue = Queue(params.num_workers * 2)
    data_queue = Queue(params.num_workers * 2)
    align_queue = Queue(params.num_workers * 2)
    write_queue = Queue(params.num_workers * 2)

    dataloader_threads = []
    for i in range(max(1, params.num_workers // 2)):
        dataloader_threads.append(
            Thread(
                target=dataloader,
                args=(
                    params,
                    cuts_queue,
                    data_queue,
                ),
            )
        )
        dataloader_threads[-1].start()

    aligner_threads = []
    for i in range(params.num_workers):
        aligner_threads.append(
            Thread(
                target=aligner,
                args=(
                    params,
                    data_queue,
                    align_queue,
                    thread_pool,
                ),
            )
        )
        aligner_threads[-1].start()

    splitter_threads = []
    for i in range(params.num_workers):
        splitter_threads.append(
            Thread(
                target=splitter,
                args=(
                    params,
                    align_queue,
                    write_queue,
                    process_pool,
                ),
            )
        )
        splitter_threads[-1].start()

    # only one thread to write results
    writer_thread = Thread(
        target=writer,
        args=(
            params,
            write_queue,
            cuts_writer,
        ),
    )
    writer_thread.start()

    batch_cuts = []
    logging.info(f"Start processing...")
    for i, cut in enumerate(raw_cuts):
        if len(batch_cuts) < params.batch_size:
            batch_cuts.append(cut)
        else:
            cuts_queue.put(batch_cuts)
            batch_cuts = []
            logging.info(f"Number of cuts have been loaded is {i}")
    if len(batch_cuts):
        cuts_queue.put(batch_cuts)
    cuts_queue.put(None)

    for t in dataloader_threads + aligner_threads + splitter_threads:
        t.join()
    writer_thread.join()
    cuts_writer.close()


if __name__ == "__main__":
    formatter = "%(asctime)s %(levelname)s [%(filename)s:%(lineno)d] %(message)s"
    logging.basicConfig(
        level=logging.DEBUG,
        format=formatter,
        handlers=[logging.FileHandler("matching.log"), logging.StreamHandler()],
    )

    main()
