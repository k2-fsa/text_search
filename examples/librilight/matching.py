import argparse
import logging
import numpy as np
import os
from pathlib import Path
from typing import List, Set, Tuple
from multiprocessing.pool import ThreadPool as Pool

from lhotse import MonoCut, CutSet, load_manifest_lazy
from lhotse.serialization import SequentialJsonlWriter
from textsearch import (
    AttributeDict,
    TextSource,
    Transcript,
    SourcedText,
    append_texts,
    create_suffix_array_from_sourced_text,
    filter_texts,
    find_candidate_matches,
    find_close_matches,
    levenshtein_distance,
    texts_to_sourced_texts,
)

PUNCTUATION = set([ord(i) for i in '():,-.!<>-?;/"'])


def get_args():
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--manifest-in",
        type=str,
        help="""The manifest generated by transcript stage containing book path,
        recordings path and recognition results.
        """,
    )
    parser.add_argument(
        "--manifest-out",
        type=str,
        help="""The file name of the new manifests to write to. 
        """,
    )
    parser.add_argument(
        "--book-dir",
        type=str,
        help="""The directory of books.
        """,
    )
    parser.add_argument(
        "--batch-size",
        type=int,
        default=10,
        help="""The number of cuts in a batch.
        """,
    )

    return parser.parse_args()


def get_params() -> AttributeDict:
    """Return a dict containing matching parameters.

    All related parameters that are not passed from the commandline
    are saved in the variable `params`.

    Commandline options are merged into `params` after they are parsed, so
    you can also access them via `params`.

    """
    params = AttributeDict(
        {
            "num_close_matches": 1,
            "num_candidates": 1,
            "match_length_ratio": 1.0,
            "use_utf8": False,
            "is_bpe": True,
            "use_uppercase": True,
        }
    )

    return params


def _align_worker(
    index: int,
    sourced_text: SourcedText,
    indexes: Tuple[int, int],
    candidate_matches: List[Tuple[int, int]],
) -> Tuple[Tuple[int, int], Tuple[int, int, str, str]]:
    query_start = sourced_text.doc_splits[index]
    query_end = sourced_text.doc_splits[index + 1]
    query = sourced_text.binary_text[query_start:query_end]
    query_length = query_end - query_start
    min_cost: float = query_length
    best_alignment: Tuple[float, List[Tuple[int, int, str]]] = None
    best_match: Tuple[int, int] = None
    for j, match in enumerate(candidate_matches):
        assert sourced_text.doc[match[0]] == sourced_text.doc[match[1]], (
            sourced_text.doc[match[0]],
            sourced_text.doc[match[1]],
        )

        start = sourced_text.doc_splits[sourced_text.doc[match[0]]]
        end = sourced_text.doc_splits[sourced_text.doc[match[0]] + 1]

        # see more text on both side
        start = max(start, match[0] - query_length // 4)
        end = min(end, match[1] + query_length // 4)

        target = sourced_text.binary_text[start:end]

        alignment = levenshtein_distance(query=query, target=target)
        if alignment[0] < min_cost:
            min_cost = alignment[0]
            best_alignment = alignment
            best_match = (start, end)

    # best_match is global index into sourced_text
    # best_alignment is local index into target
    start = best_match[0] + best_alignment[1][0][0]
    end = best_match[0] + best_alignment[1][0][1]

    # start_pos and end_pos is local index into the source text (un-normalized)
    start_pos = sourced_text.pos[start]
    end_pos = sourced_text.pos[end]

    text = "".join([chr(i) for i in query])
    assert sourced_text.doc[start] == sourced_text.doc[end]
    source = sourced_text.sources[sourced_text.doc[start]]
    ref = "".join([chr(i) for i in source.binary_text[start_pos : end_pos + 1]])

    # (ref, hyp, ref_char_pos, hyp_char_pos, hyp_time)
    aligns: List[Tuple[str, str, int, int, float]] = []
    assert sourced_text.doc[query_start] == sourced_text.doc[query_end] - 1
    query_doc_index = sourced_text.doc[query_start]
    query_source = sourced_text.sources[query_doc_index]
    q_index = 0
    t_index = start
    for i, t in enumerate(best_alignment[1][0][2]):
        hyp_time = float(query_source.times[q_index])
        if t == "I":
            aligns.append(
                (
                    "",
                    chr(query_source.binary_text[q_index]),
                    int(sourced_text.pos[t_index]),
                    q_index + 1,
                    hyp_time,
                )
            )
            q_index += 1
        elif t == "D":
            aligns.append(
                (
                    chr(sourced_text.binary_text[t_index]),
                    "",
                    int(sourced_text.pos[t_index + 1]),
                    q_index,
                    hyp_time,
                )
            )
            t_index += 1
        else:
            assert t == "E" or t == "R"
            aligns.append(
                (
                    chr(sourced_text.binary_text[t_index]),
                    chr(query_source.binary_text[q_index]),
                    int(sourced_text.pos[t_index + 1]),
                    q_index + 1,
                    hyp_time,
                )
            )
            q_index += 1
            t_index += 1

    return {"cut_index": indexes, "text": text, "ref": ref, "aligns": aligns}


def process_one_batch(
    batch_cuts: List[MonoCut], params: AttributeDict, cuts_writer: SequentialJsonlWriter
):
    """
    Process the text matching for a batch of cuts, each cuts could have multiple supervisions,
    and write the new cuts (with some extra attributes, like text, begin pos, end pos).
    """
    # List of transcripts (total number of valid supervisions in the cuts).
    transcripts: List[Transcript] = []
    # Contains cut index and local supervision index
    transcripts_cut_index: List[Tuple[int, int]] = []
    # Constructed from the valid books in the cuts
    books: List[TextSource] = []
    book_paths: Set[str] = set()
    query_len = 0

    # transcripts
    for i, cut in enumerate(batch_cuts):
        # No text book available, skip this cut.
        if cut.text_path == "":
            continue
        for j, sup in enumerate(cut.supervisions):
            # Transcript requires the input to be the dick like this.
            aligns = {"text": [], "begin_times": []}
            for ali in sup.alignment["symbol"]:
                aligns["text"].append(ali.symbol)
                aligns["begin_times"].append(ali.start)
            # alignments in a supervision might be empty
            if aligns["text"]:
                transcript = Transcript.from_dict(
                    name=sup.id,
                    d=aligns,
                    use_utf8=params.use_utf8,
                    is_bpe=params.is_bpe,
                )
                query_len += transcript.binary_text.size
                transcripts.append(transcript)
                transcripts_cut_index.append((i, j))
        book_paths.add(cut.text_path)

    # references
    for i, book_path in enumerate(book_paths):
        book_text = open(params.book_dir / book_path, "r").read()
        book = TextSource.from_str(
            name=book_path, s=book_text, use_utf8=params.use_utf8,
        )
        books.append(book)

    logging.info("Loading data done.")

    sourced_transcript_lists = texts_to_sourced_texts(
        transcripts, uppercase=params.use_uppercase
    )
    sourced_transcripts = append_texts(sourced_transcript_lists)

    sourced_book_list = texts_to_sourced_texts(books, uppercase=params.use_uppercase)
    sourced_books = append_texts(sourced_book_list)

    def _is_not_punc(c: np.int32) -> bool:
        return c not in PUNCTUATION

    sourced_books = filter_texts(sourced_books, fn=_is_not_punc)

    sourced_text = append_texts([sourced_transcripts, sourced_books])

    logging.info(f"Creating suffix array.")
    suffix_array = create_suffix_array_from_sourced_text(sourced_text)
    logging.info(f"Create suffix array done.")

    logging.info(f"Finding close matches.")
    close_matches = find_close_matches(
        suffix_array, query_len, num_close_matches=params.num_close_matches
    )
    logging.info(f"Find close matches done.")

    logging.info(f"Finding candidates.")
    candidate_matches = find_candidate_matches(
        close_matches=close_matches,
        text=sourced_text,
        num_candidates=params.num_candidates,
        length_ratio=params.match_length_ratio,
    )
    logging.info(f"Find candidates done.")

    assert len(transcripts) == len(candidate_matches), (
        len(transcripts),
        len(candidate_matches),
    )

    arguments = []
    for i in range(len(transcripts)):
        arguments.append(
            (i, sourced_text, transcripts_cut_index[i], candidate_matches[i])
        )

    num_processes = min(len(arguments), os.cpu_count())
    with Pool(num_processes) as pool:
        logging.info("Matching with levenshtein.")
        async_results = pool.starmap_async(_align_worker, arguments)
        results = async_results.get()
        logging.info("Levenshtein done.")

        valid_cuts = set()
        for res in results:
            cut_index = res["cut_index"]
            batch_cuts[cut_index[0]].supervisions[cut_index[1]].text = res["text"]
            batch_cuts[cut_index[0]].supervisions[cut_index[1]].aligns = res["aligns"]
            batch_cuts[cut_index[0]].supervisions[cut_index[1]].ref = res["ref"]
            valid_cuts.add(cut_index[0])

        logging.info(f"Writing results.")
        for index in valid_cuts:
            cuts_writer.write(batch_cuts[index], flush=True)
        logging.info(f"Write results done.")


def main():
    args = get_args()
    args.book_dir = Path(args.book_dir)
    params = get_params()
    params.update(vars(args))

    raw_cuts = load_manifest_lazy(params.manifest_in)
    cuts_writer = CutSet.open_writer(params.manifest_out, overwrite=True)
    batch_cuts = []
    logging.info(f"Start processing...")
    for i, cut in enumerate(raw_cuts):
        if len(batch_cuts) < params.batch_size:
            batch_cuts.append(cut)
        else:
            process_one_batch(batch_cuts, params, cuts_writer)
            batch_cuts = []
            logging.info(f"Number of cuts have been processed is {i}")
    if len(batch_cuts):
        process_one_batch(batch_cuts, params, cuts_writer)
    cuts_writer.close()


if __name__ == "__main__":
    formatter = "%(asctime)s %(levelname)s [%(filename)s:%(lineno)d] %(message)s"
    logging.basicConfig(
        level=logging.INFO,
        format=formatter,
        handlers=[logging.FileHandler("matching.log"), logging.StreamHandler()],
    )

    main()
